Knowledge for LLM Design
1. Requirements
1.1 Business requirement
As a mid to large size organization, it’s difficult to parse through our internal knowledge base for answers relevant to the business. For example, an engineer in a large organization looking for answer to a technical questions has to search through internal wiki’s, team chat and interal Stack Overflow to answer technical questions. Sometimes the answer is buried in a team’s groupchat. 

Our solution to this is an internal search tool that uses indexing to allow users to ask questions about their internal knowledge base. The service will allow users to add multiple data sources such as wiki links, google drive, slack group, etc. It will then index the sources and allow users to use large language models to generate summaries, ask questions and cite data from these sources.

Requirements:

As a user, I want to be able to add data from multiple sources to search from
Google drive
Slack
Internal wiki links
Drop in files
As a user, I want to be able to answer questions about my internal data
Chat bot
Search bar
As a user, I want to be able to track the source of my response 
1.2 Technical requirements
The service should provide an easy to understand interface where users can add their data sources
The service should provide an interface where users can ask questions about internal data
Chat interface
Search interface
Chat integration
The service should provide the ability to scope questions to a specific grouping
The search should yield relevant responses along with the context and source used

2. Architecture design
2.1. Data ingestion

Design source

2.2. Querying


Design source

3. User interface design
Consider
How to provide data sources under paths
How to do different query types
How to add data sources
3.1. Adding data

Design source

3.2. Querying data


Design source

4. High level design
Tenets:
Create general conceptual wrappers around specific implementation. The space is moving quickly and we don’t want to overcommit to a specific library or implementation
4.1 Components
4.1.1 Data source handler
This is a back end component with the responsibilities:
Get raw data from data sources
Handle data source permission
Send raw data to be processed
4.1.2 Raw data handler (Create index handler)
This is a back end component with the responsibilities:
Receive and process raw data
Create embeddings from raw data
Store embedding in embedding data store
Store 
Decision 1: 
Do we handle embedding storage in this handler or have a data source wrapper around the embedding store that handles storage and retrieval or embedding data?
This is a 2-way door decision.
Option 1: Handle storage in raw data handler. 
Pros
Simpler to implement
Cons
Less scalable
No separation of responsibilities
Option 2 (Recommended): Abstract embedding storage in an embedding store entity. 
Pros
Keep storing/accessing in their own place
Better abstraction
Cons
More boilerplate/development

4.2.1 Front-end client
This can be a web or mobile client. It is responsible for
Allow data source management to users
Show internal state of data sources
Receive queries and return search results

Consider
What tech we’re using
LlamaIndex
Langchain
GPT vs ChatGPT vs other LLM
How to scope queries under a certain hierarchy
Add a path under node metadata
Query for only under the path. (NEEDS RESEARCH)
How to provide sources to answers
Add a metadata tag under each node
Provide metada along with answer
How to get data from sources
Get permission from front-end, impersonate with token
